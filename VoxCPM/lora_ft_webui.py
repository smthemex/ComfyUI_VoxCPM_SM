import os
import sys
import time
import glob
import json
import yaml
import shutil
import datetime
import subprocess
import threading
import gradio as gr
import torch
import soundfile as sf
from pathlib import Path
from typing import Optional, List

# Add src to sys.path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))

# Default pretrained model path relative to this repo
default_pretrained_path = str(project_root / "models" / "openbmb__VoxCPM1.5")

from voxcpm.core import VoxCPM
from voxcpm.model.voxcpm import LoRAConfig
import numpy as np
from funasr import AutoModel

# --- Localization ---
LANG_DICT = {
    "en": {
        "title": "VoxCPM LoRA WebUI",
        "tab_train": "Training",
        "tab_infer": "Inference",
        "pretrained_path": "Pretrained Model Path",
        "train_manifest": "Train Manifest (jsonl)",
        "val_manifest": "Validation Manifest (Optional)",
        "lr": "Learning Rate",
        "max_iters": "Max Iterations",
        "batch_size": "Batch Size",
        "lora_rank": "LoRA Rank",
        "lora_alpha": "LoRA Alpha",
        "save_interval": "Save Interval",
        "start_train": "Start Training",
        "stop_train": "Stop Training",
        "train_logs": "Training Logs",
        "text_to_synth": "Text to Synthesize",
        "voice_cloning": "### Voice Cloning (Optional)",
        "ref_audio": "Reference Audio",
        "ref_text": "Reference Text (Optional)",
        "select_lora": "Select LoRA Checkpoint",
        "cfg_scale": "CFG Scale",
        "infer_steps": "Inference Steps",
        "seed": "Seed",
        "gen_audio": "Generate Audio",
        "gen_output": "Generated Audio",
        "status": "Status",
        "lang_select": "Language / è¯­è¨€",
        "refresh": "Refresh",
        "output_name": "Output Name (Optional, resume if exists)",
    },
    "zh": {
        "title": "VoxCPM LoRA WebUI",
        "tab_train": "è®­ç»ƒ (Training)",
        "tab_infer": "æ¨ç† (Inference)",
        "pretrained_path": "é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„",
        "train_manifest": "è®­ç»ƒæ•°æ®æ¸…å• (jsonl)",
        "val_manifest": "éªŒè¯æ•°æ®æ¸…å• (å¯é€‰)",
        "lr": "å­¦ä¹ ç‡ (Learning Rate)",
        "max_iters": "æœ€å¤§è¿­ä»£æ¬¡æ•°",
        "batch_size": "æ‰¹æ¬¡å¤§å° (Batch Size)",
        "lora_rank": "LoRA Rank",
        "lora_alpha": "LoRA Alpha",
        "save_interval": "ä¿å­˜é—´éš” (Steps)",
        "start_train": "å¼€å§‹è®­ç»ƒ",
        "stop_train": "åœæ­¢è®­ç»ƒ",
        "train_logs": "è®­ç»ƒæ—¥å¿—",
        "text_to_synth": "åˆæˆæ–‡æœ¬",
        "voice_cloning": "### å£°éŸ³å…‹éš† (å¯é€‰)",
        "ref_audio": "å‚è€ƒéŸ³é¢‘",
        "ref_text": "å‚è€ƒæ–‡æœ¬ (å¯é€‰)",
        "select_lora": "é€‰æ‹© LoRA æ¨¡å‹",
        "cfg_scale": "CFG Scale (å¼•å¯¼ç³»æ•°)",
        "infer_steps": "æ¨ç†æ­¥æ•°",
        "seed": "éšæœºç§å­ (Seed)",
        "gen_audio": "ç”ŸæˆéŸ³é¢‘",
        "gen_output": "ç”Ÿæˆç»“æœ",
        "status": "çŠ¶æ€",
        "lang_select": "Language / è¯­è¨€",
        "refresh": "åˆ·æ–°",
        "output_name": "è¾“å‡ºç›®å½•åç§° (å¯é€‰ï¼Œè‹¥å­˜åœ¨åˆ™ç»§ç»­è®­ç»ƒ)",
    }
}

# Global variables
current_model: Optional[VoxCPM] = None
asr_model: Optional[AutoModel] = None
training_process: Optional[subprocess.Popen] = None
training_log = ""

def get_timestamp_str():
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

def get_or_load_asr_model():
    global asr_model
    if asr_model is None:
        print("Loading ASR model (SenseVoiceSmall)...")
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        asr_model = AutoModel(
            model="iic/SenseVoiceSmall",
            disable_update=True,
            log_level='ERROR',
            device=device,
        )
    return asr_model

def recognize_audio(audio_path):
    if not audio_path:
        return ""
    try:
        model = get_or_load_asr_model()
        res = model.generate(input=audio_path, language="auto", use_itn=True)
        text = res[0]["text"].split('|>')[-1]
        return text
    except Exception as e:
        print(f"ASR Error: {e}")
        return ""

def scan_lora_checkpoints(root_dir="lora", with_info=False):
    """
    Scans for LoRA checkpoints in the lora directory.
    
    Args:
        root_dir: Directory to scan for LoRA checkpoints
        with_info: If True, returns list of (path, base_model) tuples
    
    Returns:
        List of checkpoint paths, or list of (path, base_model) tuples if with_info=True
    """
    checkpoints = []
    if not os.path.exists(root_dir):
        os.makedirs(root_dir, exist_ok=True)
    
    # Look for lora_weights.safetensors recursively
    for root, dirs, files in os.walk(root_dir):
        if "lora_weights.safetensors" in files:
            # Use the relative path from root_dir as the ID
            rel_path = os.path.relpath(root, root_dir)
            
            if with_info:
                # Try to read base_model from lora_config.json
                base_model = None
                lora_config_file = os.path.join(root, "lora_config.json")
                if os.path.exists(lora_config_file):
                    try:
                        with open(lora_config_file, "r", encoding="utf-8") as f:
                            lora_info = json.load(f)
                        base_model = lora_info.get("base_model", "Unknown")
                    except:
                        pass
                checkpoints.append((rel_path, base_model))
            else:
                checkpoints.append(rel_path)
            
    # Also check for checkpoints in the default location if they exist
    default_ckpt = "checkpoints/finetune_lora"
    if os.path.exists(os.path.join(root_dir, default_ckpt)):
         # This might be covered by the walk, but good to be sure
         pass

    return sorted(checkpoints, reverse=True)

def load_lora_config_from_checkpoint(lora_path):
    """Load LoRA config from lora_config.json if available."""
    lora_config_file = os.path.join(lora_path, "lora_config.json")
    if os.path.exists(lora_config_file):
        try:
            with open(lora_config_file, "r", encoding="utf-8") as f:
                lora_info = json.load(f)
            lora_cfg_dict = lora_info.get("lora_config", {})
            if lora_cfg_dict:
                return LoRAConfig(**lora_cfg_dict), lora_info.get("base_model")
        except Exception as e:
            print(f"Warning: Failed to load lora_config.json: {e}")
    return None, None

def get_default_lora_config():
    """Return default LoRA config for hot-swapping support."""
    return LoRAConfig(
        enable_lm=True,
        enable_dit=True,
        r=32,
        alpha=16,
        target_modules_lm=["q_proj", "v_proj", "k_proj", "o_proj"],
        target_modules_dit=["q_proj", "v_proj", "k_proj", "o_proj"]
    )

def load_model(pretrained_path, lora_path=None):
    global current_model
    print(f"Loading model from {pretrained_path}...")
    
    lora_config = None
    lora_weights_path = None
    
    if lora_path:
        full_lora_path = os.path.join("lora", lora_path)
        if os.path.exists(full_lora_path):
            lora_weights_path = full_lora_path
            # Try to load LoRA config from lora_config.json
            lora_config, _ = load_lora_config_from_checkpoint(full_lora_path)
            if lora_config:
                print(f"Loaded LoRA config from {full_lora_path}/lora_config.json")
            else:
                # Fallback to default config for old checkpoints
                lora_config = get_default_lora_config()
                print("Using default LoRA config (lora_config.json not found)")
    
    # Always init with a default LoRA config to allow hot-swapping later
    if lora_config is None:
        lora_config = get_default_lora_config()

    current_model = VoxCPM.from_pretrained(
        hf_model_id=pretrained_path,
        load_denoiser=False,
        optimize=False,
        lora_config=lora_config,
        lora_weights_path=lora_weights_path,
    )
    return "Model loaded successfully!"

def run_inference(text, prompt_wav, prompt_text, lora_selection, cfg_scale, steps, seed):
    global current_model
    
    # å¦‚æœé€‰æ‹©äº† LoRA æ¨¡å‹ä¸”å½“å‰æ¨¡å‹æœªåŠ è½½ï¼Œå°è¯•ä» LoRA config è¯»å– base_model
    if current_model is None:
        base_model_path = default_pretrained_path  # é»˜è®¤è·¯å¾„
        
        # å¦‚æœé€‰æ‹©äº† LoRAï¼Œå°è¯•ä»å…¶ config è¯»å– base_model
        if lora_selection and lora_selection != "None":
            full_lora_path = os.path.join("lora", lora_selection)
            lora_config_file = os.path.join(full_lora_path, "lora_config.json")
            
            if os.path.exists(lora_config_file):
                try:
                    with open(lora_config_file, "r", encoding="utf-8") as f:
                        lora_info = json.load(f)
                    saved_base_model = lora_info.get("base_model")
                    
                    if saved_base_model:
                        # ä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„ base_model è·¯å¾„
                        if os.path.exists(saved_base_model):
                            base_model_path = saved_base_model
                            print(f"Using base model from LoRA config: {base_model_path}")
                        else:
                            print(f"Warning: Saved base_model path not found: {saved_base_model}")
                            print(f"Falling back to default: {base_model_path}")
                except Exception as e:
                    print(f"Warning: Failed to read base_model from LoRA config: {e}")
        
        # åŠ è½½æ¨¡å‹
        try:
            print(f"Loading base model: {base_model_path}")
            status_msg = load_model(base_model_path)
            if lora_selection and lora_selection != "None":
                print(f"Model loaded for LoRA: {lora_selection}")
        except Exception as e:
            error_msg = f"Failed to load model from {base_model_path}: {str(e)}"
            print(error_msg)
            return None, error_msg

    # Handle LoRA hot-swapping
    if lora_selection and lora_selection != "None":
        full_lora_path = os.path.join("lora", lora_selection)
        print(f"Hot-loading LoRA: {full_lora_path}")
        try:
            current_model.load_lora(full_lora_path)
            current_model.set_lora_enabled(True)
        except Exception as e:
            print(f"Error loading LoRA: {e}")
            return None, f"Error loading LoRA: {e}"
    else:
        print("Disabling LoRA")
        current_model.set_lora_enabled(False)

    if seed != -1:
        torch.manual_seed(seed)
        np.random.seed(seed)

    # å¤„ç† prompt å‚æ•°ï¼šå¿…é¡»åŒæ—¶ä¸º None æˆ–åŒæ—¶æœ‰å€¼
    final_prompt_wav = None
    final_prompt_text = None
    
    if prompt_wav and prompt_wav.strip():
        # æœ‰å‚è€ƒéŸ³é¢‘
        final_prompt_wav = prompt_wav
        
        # å¦‚æœæ²¡æœ‰æä¾›å‚è€ƒæ–‡æœ¬ï¼Œå°è¯•è‡ªåŠ¨è¯†åˆ«
        if not prompt_text or not prompt_text.strip():
            print("å‚è€ƒéŸ³é¢‘å·²æä¾›ä½†ç¼ºå°‘æ–‡æœ¬ï¼Œè‡ªåŠ¨è¯†åˆ«ä¸­...")
            try:
                final_prompt_text = recognize_audio(prompt_wav)
                if final_prompt_text:
                    print(f"è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬: {final_prompt_text}")
                else:
                    return None, "é”™è¯¯ï¼šæ— æ³•è¯†åˆ«å‚è€ƒéŸ³é¢‘å†…å®¹ï¼Œè¯·æ‰‹åŠ¨å¡«å†™å‚è€ƒæ–‡æœ¬"
            except Exception as e:
                return None, f"é”™è¯¯ï¼šè‡ªåŠ¨è¯†åˆ«å‚è€ƒéŸ³é¢‘å¤±è´¥ - {str(e)}"
        else:
            final_prompt_text = prompt_text.strip()
    # å¦‚æœæ²¡æœ‰å‚è€ƒéŸ³é¢‘ï¼Œä¸¤ä¸ªéƒ½è®¾ä¸º Noneï¼ˆç”¨äºé›¶æ ·æœ¬ TTSï¼‰

    try:
        audio_np = current_model.generate(
            text=text,
            prompt_wav_path=final_prompt_wav,
            prompt_text=final_prompt_text,
            cfg_value=cfg_scale,
            inference_timesteps=steps,
            denoise=False 
        )
        return (current_model.tts_model.sample_rate, audio_np), "Generation Success"
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"Error: {str(e)}"

def start_training(
    pretrained_path,
    train_manifest,
    val_manifest,
    learning_rate,
    num_iters,
    batch_size,
    lora_rank,
    lora_alpha,
    save_interval,
    output_name="",
    # Advanced options
    grad_accum_steps=1,
    num_workers=2,
    log_interval=10,
    valid_interval=1000,
    weight_decay=0.01,
    warmup_steps=100,
    max_steps=None,
    sample_rate=44100,
    # LoRA advanced
    enable_lm=True,
    enable_dit=True,
    enable_proj=False,
    dropout=0.0,
    tensorboard_path="",
    # Distribution options
    hf_model_id="",
    distribute=False,
):
    global training_process, training_log
    
    if training_process is not None and training_process.poll() is None:
        return "Training is already running!"

    if output_name and output_name.strip():
        timestamp = output_name.strip()
    else:
        timestamp = get_timestamp_str()

    save_dir = os.path.join("lora", timestamp)
    checkpoints_dir = os.path.join(save_dir, "checkpoints")
    logs_dir = os.path.join(save_dir, "logs")
    
    os.makedirs(checkpoints_dir, exist_ok=True)
    os.makedirs(logs_dir, exist_ok=True)

    # Create config dictionary
    # Resolve max_steps default
    resolved_max_steps = int(max_steps) if max_steps not in (None, "", 0) else int(num_iters)

    config = {
        "pretrained_path": pretrained_path,
        "train_manifest": train_manifest,
        "val_manifest": val_manifest,
        "sample_rate": int(sample_rate),
        "batch_size": int(batch_size),
        "grad_accum_steps": int(grad_accum_steps),
        "num_workers": int(num_workers),
        "num_iters": int(num_iters),
        "log_interval": int(log_interval),
        "valid_interval": int(valid_interval),
        "save_interval": int(save_interval),
        "learning_rate": float(learning_rate),
        "weight_decay": float(weight_decay),
        "warmup_steps": int(warmup_steps),
        "max_steps": resolved_max_steps,
        "save_path": checkpoints_dir,
        "tensorboard": tensorboard_path if tensorboard_path else logs_dir,
        "lambdas": {
            "loss/diff": 1.0,
            "loss/stop": 1.0
        },
        "lora": {
            "enable_lm": bool(enable_lm),
            "enable_dit": bool(enable_dit),
            "enable_proj": bool(enable_proj),
            "r": int(lora_rank),
            "alpha": int(lora_alpha),
            "dropout": float(dropout),
            "target_modules_lm": ["q_proj", "v_proj", "k_proj", "o_proj"],
            "target_modules_dit": ["q_proj", "v_proj", "k_proj", "o_proj"]
        },
    }
    
    # Add distribution options if provided
    if hf_model_id and hf_model_id.strip():
        config["hf_model_id"] = hf_model_id.strip()
    if distribute:
        config["distribute"] = True

    config_path = os.path.join(save_dir, "train_config.yaml")
    with open(config_path, "w") as f:
        yaml.dump(config, f)

    cmd = [
        sys.executable,
        "scripts/train_voxcpm_finetune.py",
        "--config_path",
        config_path
    ]

    training_log = f"Starting training...\nConfig saved to {config_path}\nOutput dir: {save_dir}\n"
    
    def run_process():
        global training_process, training_log
        training_process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )
        
        for line in training_process.stdout:
            training_log += line
            # Keep log size manageable
            if len(training_log) > 100000:
                training_log = training_log[-100000:]
        
        training_process.wait()
        training_log += f"\nTraining finished with code {training_process.returncode}"

    threading.Thread(target=run_process, daemon=True).start()
    
    return f"Training started! Check 'lora/{timestamp}'"

def get_training_log():
    return training_log

def stop_training():
    global training_process, training_log
    if training_process is not None and training_process.poll() is None:
        training_process.terminate()
        training_log += "\nTraining terminated by user."
        return "Training stopped."
    return "No training running."

# --- GUI Layout ---

# è‡ªå®šä¹‰CSSæ ·å¼
custom_css = """
/* æ•´ä½“ä¸»é¢˜æ ·å¼ */
.gradio-container {
    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

/* æ ‡é¢˜åŒºåŸŸæ ·å¼ - æ‰å¹³åŒ–è®¾è®¡ */
.title-section {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    border-radius: 8px;
    padding: 15px 25px;
    margin-bottom: 15px;
    border: none;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

.title-section h1 {
    color: white;
    text-shadow: none;
    font-weight: 600;
    margin: 0;
    font-size: 28px;
    line-height: 1.2;
}

.title-section h3 {
    color: rgba(255, 255, 255, 0.9);
    font-weight: 400;
    margin-top: 5px;
    font-size: 14px;
    line-height: 1.3;
}

.title-section p {
    color: rgba(255, 255, 255, 0.85);
    font-size: 13px;
    margin: 5px 0 0 0;
    line-height: 1.3;
}

/* æ ‡ç­¾é¡µæ ·å¼ */
.tabs {
    background: white;
    border-radius: 15px;
    padding: 10px;
    box-shadow: 0 4px 20px rgba(0,0,0,0.08);
}

/* æŒ‰é’®æ ·å¼å¢å¼º */
.button-primary {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    border: none;
    border-radius: 12px;
    padding: 12px 30px;
    font-weight: 600;
    color: white;
    transition: all 0.3s ease;
    box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
}

.button-primary:hover {
    transform: translateY(-2px);
    box-shadow: 0 6px 25px rgba(102, 126, 234, 0.4);
}

.button-stop {
    background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
    border: none;
    border-radius: 12px;
    padding: 12px 30px;
    font-weight: 600;
    color: white;
    transition: all 0.3s ease;
    box-shadow: 0 4px 15px rgba(250, 112, 154, 0.3);
}

.button-stop:hover {
    transform: translateY(-2px);
    box-shadow: 0 6px 25px rgba(250, 112, 154, 0.4);
}

.button-refresh {
    background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
    border: none;
    border-radius: 10px;
    padding: 8px 20px;
    font-weight: 500;
    color: white;
    transition: all 0.3s ease;
    box-shadow: 0 2px 10px rgba(132, 250, 176, 0.3);
}

.button-refresh:hover {
    transform: translateY(-1px);
    box-shadow: 0 4px 15px rgba(132, 250, 176, 0.4);
}

/* è¡¨å•åŒºåŸŸæ ·å¼ */
.form-section {
    background: white;
    border-radius: 20px;
    padding: 30px;
    margin: 15px 0;
    box-shadow: 0 8px 30px rgba(0,0,0,0.08);
    border: 1px solid rgba(0,0,0,0.05);
}

/* è¾“å…¥æ¡†æ ·å¼ */
.input-field {
    border-radius: 12px;
    border: 2px solid #e0e0e0;
    padding: 12px 16px;
    transition: all 0.3s ease;
    background: #fafafa;
}

.input-field:focus {
    border-color: #667eea;
    box-shadow: 0 0 0 4px rgba(102, 126, 234, 0.1);
    background: white;
}

/* æ»‘å—æ ·å¼ */
.slider {
    -webkit-appearance: none;
    appearance: none;
    width: 100%;
    height: 6px;
    border-radius: 3px;
    background: linear-gradient(90deg, #667eea, #764ba2);
    outline: none;
    opacity: 0.8;
    transition: opacity 0.2s;
}

.slider:hover {
    opacity: 1;
}

.slider::-webkit-slider-thumb {
    -webkit-appearance: none;
    appearance: none;
    width: 18px;
    height: 18px;
    border-radius: 50%;
    background: white;
    cursor: pointer;
    border: 3px solid #667eea;
    box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3);
}

.slider::-moz-range-thumb {
    width: 18px;
    height: 18px;
    border-radius: 50%;
    background: white;
    cursor: pointer;
    border: 3px solid #667eea;
    box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3);
}

/* æŠ˜å é¢æ¿æ ·å¼ */
.accordion {
    border-radius: 12px;
    border: 2px solid #e0e0e0;
    overflow: hidden;
    background: white;
}

.accordion-header {
    background: linear-gradient(135deg, #f5f7fa 0%, #e3e7ed 100%);
    padding: 15px 20px;
    font-weight: 600;
    color: #333;
}

/* çŠ¶æ€æ˜¾ç¤ºæ ·å¼ */
.status-success {
    background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
    color: white;
    padding: 12px 20px;
    border-radius: 12px;
    font-weight: 500;
    box-shadow: 0 4px 15px rgba(132, 250, 176, 0.3);
}

.status-error {
    background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
    color: white;
    padding: 12px 20px;
    border-radius: 12px;
    font-weight: 500;
    box-shadow: 0 4px 15px rgba(250, 112, 154, 0.3);
}

/* è¯­è¨€åˆ‡æ¢æŒ‰é’®æ ·å¼ - æ‰å¹³åŒ– */
.lang-selector {
    background: rgba(255, 255, 255, 0.25);
    backdrop-filter: blur(10px);
    border-radius: 8px;
    padding: 8px 12px;
    border: 1px solid rgba(255, 255, 255, 0.4);
}

.lang-selector label.gr-box {
    color: white !important;
    font-weight: 600;
    margin-bottom: 8px !important;
}

/* å•é€‰æŒ‰é’®ç»„æ ·å¼ */
.lang-selector fieldset,
.lang-selector .gr-form {
    gap: 10px !important;
    display: flex !important;
}

/* å•é€‰æŒ‰é’®å®¹å™¨ - æ‰å¹³åŒ– (æœªé€‰ä¸­çŠ¶æ€ - è¾ƒæµ…çš„æ·±è‰²) */
.lang-selector label.gr-radio-label {
    background: linear-gradient(135deg, rgba(102, 126, 234, 0.6), rgba(118, 75, 162, 0.6)) !important;
    border: 1px solid rgba(255, 255, 255, 0.5) !important;
    border-radius: 6px !important;
    padding: 8px 18px !important;
    color: white !important;
    font-weight: 500 !important;
    transition: all 0.2s ease !important;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1) !important;
    cursor: pointer !important;
    margin: 0 4px !important;
}

/* é€‰ä¸­çš„å•é€‰æŒ‰é’® - æ‰å¹³åŒ– (æ›´æ·±çš„æ·±è‰²èƒŒæ™¯) */
.lang-selector input[type="radio"]:checked + label,
.lang-selector label.gr-radio-label:has(input:checked) {
    background: linear-gradient(135deg, #5568d3, #6b4c9a) !important;
    color: white !important;
    border: 1px solid rgba(255, 255, 255, 0.6) !important;
    font-weight: 600 !important;
    box-shadow: 0 3px 12px rgba(0, 0, 0, 0.2) !important;
    transform: none !important;
}

/* æœªé€‰ä¸­çš„å•é€‰æŒ‰é’®æ‚¬åœæ•ˆæœ - æ‰å¹³åŒ– */
.lang-selector label.gr-radio-label:hover {
    background: linear-gradient(135deg, rgba(102, 126, 234, 0.75), rgba(118, 75, 162, 0.75)) !important;
    border-color: rgba(255, 255, 255, 0.7) !important;
    transform: none !important;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.15) !important;
}

/* éšè—åŸå§‹çš„å•é€‰æŒ‰é’®åœ†ç‚¹ */
.lang-selector input[type="radio"] {
    opacity: 0;
    position: absolute;
}

/* Gradio Radio ç‰¹å®šæ ·å¼ - æ‰å¹³åŒ– */
.lang-selector .wrap {
    gap: 8px !important;
}

.lang-selector .wrap > label {
    background: linear-gradient(135deg, rgba(102, 126, 234, 0.6), rgba(118, 75, 162, 0.6)) !important;
    border: 1px solid rgba(255, 255, 255, 0.5) !important;
    border-radius: 6px !important;
    padding: 8px 18px !important;
    color: white !important;
    font-weight: 500 !important;
    transition: all 0.2s ease !important;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1) !important;
}

.lang-selector .wrap > label.selected {
    background: linear-gradient(135deg, #5568d3, #6b4c9a) !important;
    color: white !important;
    border: 1px solid rgba(255, 255, 255, 0.6) !important;
    font-weight: 600 !important;
    box-shadow: 0 3px 12px rgba(0, 0, 0, 0.2) !important;
}

/* æ ‡ç­¾æ ·å¼ä¼˜åŒ– */
label {
    color: #333;
    font-weight: 500;
    margin-bottom: 8px;
}

/* Markdown æ ‡é¢˜æ ·å¼ */
.markdown-text h4 {
    color: #667eea;
    font-weight: 600;
    margin-top: 15px;
    margin-bottom: 10px;
}

/* å‚æ•°ç»„ä»¶é—´è·ä¼˜åŒ– */
.form-section > div {
    margin-bottom: 15px;
}

/* Slider ç»„ä»¶æ ·å¼ä¼˜åŒ– */
.gr-slider {
    padding: 10px 0;
}

/* Number è¾“å…¥æ¡†ä¼˜åŒ– */
.gr-number {
    max-width: 100%;
}

/* æŒ‰é’®å®¹å™¨ä¼˜åŒ– */
.gr-button {
    min-height: 45px;
    font-size: 16px;
}

/* ä¸‰æ å¸ƒå±€ä¼˜åŒ– */
#component-0 .gr-row {
    gap: 20px;
}

/* ç”ŸæˆæŒ‰é’®ç‰¹æ®Šæ ·å¼ */
.button-primary.gr-button-lg {
    min-height: 55px;
    font-size: 18px;
    font-weight: 700;
    margin-top: 20px;
    margin-bottom: 10px;
}

/* åˆ·æ–°æŒ‰é’®å°å°ºå¯¸ */
.button-refresh.gr-button-sm {
    min-height: 38px;
    font-size: 14px;
    margin-top: 5px;
    margin-bottom: 15px;
}

/* ä¿¡æ¯æç¤ºæ–‡å­—æ ·å¼ */
.gr-info {
    font-size: 13px;
    color: #666;
    margin-top: 5px;
}

/* åŒºåŸŸæ ‡é¢˜æ ·å¼ä¼˜åŒ– */
.form-section h4 {
    color: #667eea;
    font-weight: 600;
    margin-top: 0;
    margin-bottom: 15px;
    padding-bottom: 10px;
    border-bottom: 2px solid #f0f0f0;
}

.form-section strong {
    color: #667eea;
    font-size: 15px;
    display: block;
    margin: 15px 0 10px 0;
}
"""

with gr.Blocks(
    title="VoxCPM LoRA WebUI",
    theme=gr.themes.Soft(),
    css=custom_css
) as app:
    
    # State for language
    lang_state = gr.State("zh") # Default to Chinese

    # æ ‡é¢˜åŒºåŸŸ
    with gr.Row(elem_classes="title-section"):
        with gr.Column(scale=3):
            title_md = gr.Markdown("""
            # ğŸµ VoxCPM LoRA WebUI
            ### å¼ºå¤§çš„è¯­éŸ³åˆæˆå’Œ LoRA å¾®è°ƒå·¥å…·

            æ”¯æŒè¯­éŸ³å…‹éš†ã€LoRA æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
            """)
        with gr.Column(scale=1):
            lang_btn = gr.Radio(
                choices=["en", "zh"],
                value="zh",
                label="ğŸŒ Language / è¯­è¨€",
                elem_classes="lang-selector"
            )

    with gr.Tabs(elem_classes="tabs") as tabs:
        # === Training Tab ===
        with gr.Tab("ğŸš€ è®­ç»ƒ (Training)") as tab_train:
            gr.Markdown("""
            ### ğŸ¯ æ¨¡å‹è®­ç»ƒè®¾ç½®
            é…ç½®ä½ çš„ LoRA å¾®è°ƒè®­ç»ƒå‚æ•°
            """)

            with gr.Row():
                with gr.Column(scale=2, elem_classes="form-section"):
                    gr.Markdown("#### ğŸ“ åŸºç¡€é…ç½®")

                    train_pretrained_path = gr.Textbox(
                        label="ğŸ“‚ é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„",
                        value=default_pretrained_path,
                        elem_classes="input-field"
                    )
                    train_manifest = gr.Textbox(
                        label="ğŸ“‹ è®­ç»ƒæ•°æ®æ¸…å• (jsonl)",
                        value="examples/train_data_example.jsonl",
                        elem_classes="input-field"
                    )
                    val_manifest = gr.Textbox(
                        label="ğŸ“Š éªŒè¯æ•°æ®æ¸…å• (å¯é€‰)",
                        value="",
                        elem_classes="input-field"
                    )

                    gr.Markdown("#### âš™ï¸ è®­ç»ƒå‚æ•°")

                    with gr.Row():
                        lr = gr.Number(
                            label="ğŸ“ˆ å­¦ä¹ ç‡ (Learning Rate)",
                            value=1e-4,
                            elem_classes="input-field"
                        )
                        num_iters = gr.Number(
                            label="ğŸ”„ æœ€å¤§è¿­ä»£æ¬¡æ•°",
                            value=2000,
                            precision=0,
                            elem_classes="input-field"
                        )
                        batch_size = gr.Number(
                            label="ğŸ“¦ æ‰¹æ¬¡å¤§å° (Batch Size)",
                            value=1,
                            precision=0,
                            elem_classes="input-field"
                        )

                    with gr.Row():
                        lora_rank = gr.Number(
                            label="ğŸ¯ LoRA Rank",
                            value=32,
                            precision=0,
                            elem_classes="input-field"
                        )
                        lora_alpha = gr.Number(
                            label="âš–ï¸ LoRA Alpha",
                            value=16,
                            precision=0,
                            elem_classes="input-field"
                        )
                        save_interval = gr.Number(
                            label="ğŸ’¾ ä¿å­˜é—´éš” (Steps)",
                            value=1000,
                            precision=0,
                            elem_classes="input-field"
                        )

                    output_name = gr.Textbox(
                        label="ğŸ“ è¾“å‡ºç›®å½•åç§° (å¯é€‰ï¼Œè‹¥å­˜åœ¨åˆ™ç»§ç»­è®­ç»ƒ)",
                        value="",
                        elem_classes="input-field"
                    )

                    with gr.Row():
                        start_btn = gr.Button(
                            "â–¶ï¸ å¼€å§‹è®­ç»ƒ",
                            variant="primary",
                            elem_classes="button-primary"
                        )
                        stop_btn = gr.Button(
                            "â¹ï¸ åœæ­¢è®­ç»ƒ",
                            variant="stop",
                            elem_classes="button-stop"
                        )

                    with gr.Accordion("ğŸ”§ é«˜çº§é€‰é¡¹ (Advanced)", open=False, elem_classes="accordion"):
                        with gr.Row():
                            grad_accum_steps = gr.Number(label="æ¢¯åº¦ç´¯ç§¯ (grad_accum_steps)", value=1, precision=0)
                            num_workers = gr.Number(label="æ•°æ®åŠ è½½çº¿ç¨‹ (num_workers)", value=2, precision=0)
                            log_interval = gr.Number(label="æ—¥å¿—é—´éš” (log_interval)", value=10, precision=0)
                        with gr.Row():
                            valid_interval = gr.Number(label="éªŒè¯é—´éš” (valid_interval)", value=1000, precision=0)
                            weight_decay = gr.Number(label="æƒé‡è¡°å‡ (weight_decay)", value=0.01)
                            warmup_steps = gr.Number(label="warmup_steps", value=100, precision=0)
                        with gr.Row():
                            max_steps = gr.Number(label="æœ€å¤§æ­¥æ•° (max_steps, 0â†’é»˜è®¤num_iters)", value=0, precision=0)
                            sample_rate = gr.Number(label="é‡‡æ ·ç‡ (sample_rate)", value=44100, precision=0)
                            tensorboard_path = gr.Textbox(label="Tensorboard è·¯å¾„ (å¯é€‰)", value="")
                        with gr.Row():
                            enable_lm = gr.Checkbox(label="å¯ç”¨ LoRA LM (enable_lm)", value=True)
                            enable_dit = gr.Checkbox(label="å¯ç”¨ LoRA DIT (enable_dit)", value=True)
                            enable_proj = gr.Checkbox(label="å¯ç”¨æŠ•å½± (enable_proj)", value=False)
                            dropout = gr.Number(label="LoRA Dropout", value=0.0)
                        
                        gr.Markdown("#### åˆ†å‘é€‰é¡¹ (Distribution)")
                        with gr.Row():
                            hf_model_id = gr.Textbox(label="HuggingFace Model ID (e.g., openbmb/VoxCPM1.5)", value="openbmb/VoxCPM1.5")
                            distribute = gr.Checkbox(label="åˆ†å‘æ¨¡å¼ (distribute)", value=False)

                with gr.Column(scale=2, elem_classes="form-section"):
                    gr.Markdown("#### ğŸ“Š è®­ç»ƒæ—¥å¿—")
                    logs_out = gr.TextArea(
                        label="",
                        lines=20,
                        max_lines=30,
                        interactive=False,
                        elem_classes="input-field",
                        show_label=False
                    )
                    
            start_btn.click(
                start_training,
                inputs=[
                    train_pretrained_path, train_manifest, val_manifest,
                    lr, num_iters, batch_size, lora_rank, lora_alpha, save_interval,
                    output_name,
                    # advanced
                    grad_accum_steps, num_workers, log_interval, valid_interval,
                    weight_decay, warmup_steps, max_steps, sample_rate,
                    enable_lm, enable_dit, enable_proj, dropout, tensorboard_path,
                    # distribution
                    hf_model_id, distribute
                ],
                outputs=[logs_out] # Initial message
            )
            stop_btn.click(stop_training, outputs=[logs_out])
            
            # Log refresher
            timer = gr.Timer(1)
            timer.tick(get_training_log, outputs=logs_out)

        # === Inference Tab ===
        with gr.Tab("ğŸµ æ¨ç† (Inference)") as tab_infer:
            gr.Markdown("""
            ### ğŸ¤ è¯­éŸ³åˆæˆ
            ä½¿ç”¨è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹ç”Ÿæˆè¯­éŸ³ï¼Œæ”¯æŒ LoRA å¾®è°ƒå’Œå£°éŸ³å…‹éš†
            """)

            with gr.Row():
                # å·¦æ ï¼šè¾“å…¥é…ç½® (35%)
                with gr.Column(scale=35, elem_classes="form-section"):
                    gr.Markdown("#### ğŸ“ è¾“å…¥é…ç½®")

                    infer_text = gr.TextArea(
                        label="ğŸ’¬ åˆæˆæ–‡æœ¬",
                        value="Hello, this is a test of the VoxCPM LoRA model.",
                        elem_classes="input-field",
                        lines=4,
                        placeholder="è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬å†…å®¹..."
                    )

                    gr.Markdown("**ğŸ­ å£°éŸ³å…‹éš†ï¼ˆå¯é€‰ï¼‰**")
                    
                    prompt_wav = gr.Audio(
                        label="ğŸµ å‚è€ƒéŸ³é¢‘",
                        type="filepath",
                        elem_classes="input-field"
                    )
                    
                    prompt_text = gr.Textbox(
                        label="ğŸ“ å‚è€ƒæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰",
                        elem_classes="input-field",
                        placeholder="å¦‚ä¸å¡«å†™ï¼Œå°†è‡ªåŠ¨è¯†åˆ«å‚è€ƒéŸ³é¢‘å†…å®¹"
                    )

                # ä¸­æ ï¼šæ¨¡å‹é€‰æ‹©å’Œå‚æ•°é…ç½® (35%)
                with gr.Column(scale=35, elem_classes="form-section"):
                    gr.Markdown("#### ğŸ¤– æ¨¡å‹é€‰æ‹©")

                    lora_select = gr.Dropdown(
                        label="ğŸ¯ LoRA æ¨¡å‹",
                        choices=["None"] + scan_lora_checkpoints(),
                        value="None",
                        interactive=True,
                        elem_classes="input-field",
                        info="é€‰æ‹©è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹ï¼Œæˆ–é€‰æ‹© None ä½¿ç”¨åŸºç¡€æ¨¡å‹"
                    )
                    
                    refresh_lora_btn = gr.Button(
                        "ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨",
                        elem_classes="button-refresh",
                        size="sm"
                    )

                    gr.Markdown("#### âš™ï¸ ç”Ÿæˆå‚æ•°")

                    cfg_scale = gr.Slider(
                        label="ğŸ›ï¸ CFG Scale",
                        minimum=1.0,
                        maximum=5.0,
                        value=2.0,
                        step=0.1,
                        info="å¼•å¯¼ç³»æ•°ï¼Œå€¼è¶Šå¤§è¶Šè´´è¿‘æç¤º"
                    )
                    
                    steps = gr.Slider(
                        label="ğŸ”¢ æ¨ç†æ­¥æ•°",
                        minimum=1,
                        maximum=50,
                        value=10,
                        step=1,
                        info="ç”Ÿæˆè´¨é‡ä¸æ­¥æ•°æˆæ­£æ¯”ï¼Œä½†è€—æ—¶æ›´é•¿"
                    )
                    
                    seed = gr.Number(
                        label="ğŸ² éšæœºç§å­",
                        value=-1,
                        precision=0,
                        elem_classes="input-field",
                        info="-1 ä¸ºéšæœºï¼Œå›ºå®šå€¼å¯å¤ç°ç»“æœ"
                    )

                    generate_btn = gr.Button(
                        "ğŸµ ç”ŸæˆéŸ³é¢‘",
                        variant="primary",
                        elem_classes="button-primary",
                        size="lg"
                    )

                # å³æ ï¼šç”Ÿæˆç»“æœ (30%)
                with gr.Column(scale=30, elem_classes="form-section"):
                    gr.Markdown("#### ğŸ§ ç”Ÿæˆç»“æœ")
                    
                    audio_out = gr.Audio(
                        label="",
                        elem_classes="input-field",
                        show_label=False
                    )

                    gr.Markdown("#### ğŸ“‹ çŠ¶æ€ä¿¡æ¯")
                    
                    status_out = gr.Textbox(
                        label="",
                        interactive=False,
                        elem_classes="input-field",
                        show_label=False,
                        lines=3,
                        placeholder="ç­‰å¾…ç”Ÿæˆ..."
                    )

            def refresh_loras():
                # è·å– LoRA checkpoints åŠå…¶ base model ä¿¡æ¯
                checkpoints_with_info = scan_lora_checkpoints(with_info=True)
                choices = ["None"] + [ckpt[0] for ckpt in checkpoints_with_info]
                
                # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
                print(f"åˆ·æ–° LoRA åˆ—è¡¨: æ‰¾åˆ° {len(checkpoints_with_info)} ä¸ªæ£€æŸ¥ç‚¹")
                for ckpt_path, base_model in checkpoints_with_info:
                    if base_model:
                        print(f"  - {ckpt_path} (Base Model: {base_model})")
                    else:
                        print(f"  - {ckpt_path}")
                
                return gr.update(choices=choices, value="None")

            refresh_lora_btn.click(refresh_loras, outputs=[lora_select])
            
            # Auto-recognize audio when uploaded
            prompt_wav.change(
                fn=recognize_audio,
                inputs=[prompt_wav],
                outputs=[prompt_text]
            )
            
            generate_btn.click(
                run_inference,
                inputs=[infer_text, prompt_wav, prompt_text, lora_select, cfg_scale, steps, seed],
                outputs=[audio_out, status_out]
            )

    # --- Language Switching Logic ---
    def change_language(lang):
        d = LANG_DICT[lang]
        # Labels for advanced options
        if lang == "zh":
            adv = {
                'grad_accum_steps': "æ¢¯åº¦ç´¯ç§¯ (grad_accum_steps)",
                'num_workers': "æ•°æ®åŠ è½½çº¿ç¨‹ (num_workers)",
                'log_interval': "æ—¥å¿—é—´éš” (log_interval)",
                'valid_interval': "éªŒè¯é—´éš” (valid_interval)",
                'weight_decay': "æƒé‡è¡°å‡ (weight_decay)",
                'warmup_steps': "warmup_steps",
                'max_steps': "æœ€å¤§æ­¥æ•° (max_steps)",
                'sample_rate': "é‡‡æ ·ç‡ (sample_rate)",
                'enable_lm': "å¯ç”¨ LoRA LM (enable_lm)",
                'enable_dit': "å¯ç”¨ LoRA DIT (enable_dit)",
                'enable_proj': "å¯ç”¨æŠ•å½± (enable_proj)",
                'dropout': "LoRA Dropout",
                'tensorboard_path': "Tensorboard è·¯å¾„ (å¯é€‰)",
                'hf_model_id': "HuggingFace Model ID (e.g., openbmb/VoxCPM1.5)",
                'distribute': "åˆ†å‘æ¨¡å¼ (distribute)",
            }
        else:
            adv = {
                'grad_accum_steps': "Grad Accum Steps",
                'num_workers': "Num Workers",
                'log_interval': "Log Interval",
                'valid_interval': "Valid Interval",
                'weight_decay': "Weight Decay",
                'warmup_steps': "Warmup Steps",
                'max_steps': "Max Steps",
                'sample_rate': "Sample Rate",
                'enable_lm': "Enable LoRA LM",
                'enable_dit': "Enable LoRA DIT",
                'enable_proj': "Enable Projection",
                'dropout': "LoRA Dropout",
                'tensorboard_path': "Tensorboard Path (Optional)",
                'hf_model_id': "HuggingFace Model ID (e.g., openbmb/VoxCPM1.5)",
                'distribute': "Distribute Mode",
            }

        return (
            gr.update(value=f"# {d['title']}"),
            gr.update(label=d['tab_train']),
            gr.update(label=d['tab_infer']),
            gr.update(label=d['pretrained_path']),
            gr.update(label=d['train_manifest']),
            gr.update(label=d['val_manifest']),
            gr.update(label=d['lr']),
            gr.update(label=d['max_iters']),
            gr.update(label=d['batch_size']),
            gr.update(label=d['lora_rank']),
            gr.update(label=d['lora_alpha']),
            gr.update(label=d['save_interval']),
            gr.update(label=d['output_name']),
            gr.update(value=d['start_train']),
            gr.update(value=d['stop_train']),
            gr.update(label=d['train_logs']),
            # Advanced options (must match outputs order)
            gr.update(label=adv['grad_accum_steps']),
            gr.update(label=adv['num_workers']),
            gr.update(label=adv['log_interval']),
            gr.update(label=adv['valid_interval']),
            gr.update(label=adv['weight_decay']),
            gr.update(label=adv['warmup_steps']),
            gr.update(label=adv['max_steps']),
            gr.update(label=adv['sample_rate']),
            gr.update(label=adv['enable_lm']),
            gr.update(label=adv['enable_dit']),
            gr.update(label=adv['enable_proj']),
            gr.update(label=adv['dropout']),
            gr.update(label=adv['tensorboard_path']),
            # Distribution options
            gr.update(label=adv['hf_model_id']),
            gr.update(label=adv['distribute']),
            # Inference section
            gr.update(label=d['text_to_synth']),
            gr.update(label=d['ref_audio']),
            gr.update(label=d['ref_text']),
            gr.update(label=d['select_lora']),
            gr.update(value=d['refresh']),
            gr.update(label=d['cfg_scale']),
            gr.update(label=d['infer_steps']),
            gr.update(label=d['seed']),
            gr.update(value=d['gen_audio']),
            gr.update(label=d['gen_output']),
            gr.update(label=d['status']),
        )

    lang_btn.change(
        change_language,
        inputs=[lang_btn],
        outputs=[
            title_md, tab_train, tab_infer,
            train_pretrained_path, train_manifest, val_manifest,
            lr, num_iters, batch_size, lora_rank, lora_alpha, save_interval,
            output_name,
            start_btn, stop_btn, logs_out,
            # advanced outputs
            grad_accum_steps, num_workers, log_interval, valid_interval,
            weight_decay, warmup_steps, max_steps, sample_rate,
            enable_lm, enable_dit, enable_proj, dropout, tensorboard_path,
            # distribution outputs
            hf_model_id, distribute,
            infer_text, prompt_wav, prompt_text,
            lora_select, refresh_lora_btn, cfg_scale, steps, seed,
            generate_btn, audio_out, status_out
        ]
    )

if __name__ == "__main__":
    # Ensure lora directory exists
    os.makedirs("lora", exist_ok=True)
    app.queue().launch(server_name="0.0.0.0", server_port=7860)